<p dir="ltr"><strong id="docs-internal-guid-7f62b13a-871a-9046-f28d-0dac3fa6e43e">Manual Annotation: </strong>Manual annotation of the dataset to obtain real-valued scores was done through Best-Worst Scaling (BWS), an annotation scheme shown to obtain very reliable scores (Kiritchenko and Mohammad, 2016). The data is then split into a training set and a test set. The test set released at the start of the evaluation period will not include the real-valued sentiment scores. These scores for the test data, which we will refer to as the <em>Gold data</em>, will be released after evaluation, when the results are posted.</p>
<p dir="ltr">The emotion intensity scores for both training and test data are obtained by crowdsourcing. Standard crowdsourcing best practices were followed such as pre-annotating 5% to 10% of questions internally (by one of the task organizers). These pre-annnotations were used to randomly check quality of crowdsourced responses and inform annotators of errors as and when they make them. (This has been shown to significantly improve annotation quality).</p>

<p dir="ltr"><strong id="docs-internal-guid-7f62b13a-8770-0b7a-fc4b-5f3ad9a50eb0">Best-Worst Scaling</strong> <strong>Questionnaires and Directions to Annotators</strong></p>
<p dir="ltr">Obtaining real-valued sentiment annotations has several challenges. Respondents are faced with a higher cognitive load when asked for real-valued sentiment scores for terms as opposed to simply classifying terms as either positive or negative. It is also difficult for an annotator to remain consistent with his/her annotations. Further, the same sentiment association may map to different sentiment scores in the minds of different annotators; for example, one annotator may assign a score of 0.6 and another 0.8 for the same degree of positive association. One could overcome these problems by providing annotators with pairs of terms and asking which is more positive (a comparative approach), however that requires a much larger set of annotations (order N2, where N is the number of terms to be annotated).</p>
<p dir="ltr">Best-Worst Scaling (BWS), also sometimes referred to as Maximum Difference Scaling (MaxDiff), is an annotation scheme that exploits the comparative approach to annotation (Louviere and Woodworth, 1990; Cohen, 2003; Louviere et al., 2015; Kiritchenko and Mohammad, 2016) while still keeping the number of required annotations small. Annotators are given four items (4-tuple) and asked which item is the Best (highest in terms of the property of interest) and which is the Worst (least in terms of the property of interest). These annotations can then be easily converted into real-valued scores of association between the items and the property, which eventually allows for creating a ranked list of items as per their association with the property of interest.</p>
<p dir="ltr">The questionnaires used to annotate the data are available here:</p>
<ul>
<li>for <a href="http://saifmohammad.com/WebDocs/anger-BWS-questionnaire.pdf">anger</a></li>
<li>for <a href="http://saifmohammad.com/WebDocs/fear-BWS-questionnaire.pdf">fear</a></li>
<li>for <a href="http://saifmohammad.com/WebDocs/joy-BWS-questionnaire.pdf">joy</a></li>
<li>for <a href="http://saifmohammad.com/WebDocs/sadness-BWS-questionnaire.pdf">sadness</a></li>
</ul>